# -*- coding: utf-8 -*-
from pyspark.sql import SparkSession
# explode, pandas_udf ë“± ìƒˆë¡œìš´ í•¨ìˆ˜ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
from pyspark.sql.functions import col, length, regexp_replace, udf, explode, pandas_udf
from pyspark.sql.types import ArrayType, StringType, StructType, StructField, FloatType
import pandas as pd

def main():
    """
    Main function to run the data processing pipeline.
    """
    # pipeline.py
    # ë¡œì»¬ì—ì„œ 4ê°œì˜ ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ Spark ì„¸ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
    spark = SparkSession.builder \
        .appName("K-Culturefy-Pipeline") \
        .master("local[4]") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "1g") \
        .getOrCreate()

    print("Spark ì„¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("url", StringType(), True),
        StructField("title", StringType(), True),
        StructField("text", StringType(), True)
    ])

    # --- ğŸ‘‡ [ìˆ˜ì •ë¨] ê²½ë¡œë¥¼ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œë¡œ ë³€ê²½ ---
    wiki_df = spark.read.schema(schema).json("/app/extracted_text/*/*")
    
    print("ë°ì´í„° ì •ì œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    cleaned_df = wiki_df.filter(
        (col("text").isNotNull()) & (length(col("text")) > 100)
    ).withColumn("text", regexp_replace(col("text"), "[\n\r]", " "))

    cleaned_df.cache()
    cleaned_count = cleaned_df.count()
    print(f"ì •ì œ í›„ ë‚¨ì€ ë¬¸ì„œ ê°œìˆ˜: {cleaned_count}")

    print("\nì˜ë¯¸ ê¸°ë°˜ ë¶„í• (Chunking)ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    def semantic_chunking(text):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        chunk_size = 1000

        for p in paragraphs:
            trimmed_p = p.strip()
            if not trimmed_p: continue
            if len(current_chunk) + len(trimmed_p) + 2 > chunk_size and current_chunk:
                chunks.append(current_chunk)
                current_chunk = ""
            current_chunk = f"{current_chunk}\n\n{trimmed_p}" if current_chunk else trimmed_p
        if current_chunk: chunks.append(current_chunk)
        return chunks

    chunking_udf = udf(semantic_chunking, ArrayType(StringType()))

    chunked_df = cleaned_df.withColumn("chunks", chunking_udf(cleaned_df["text"]))

    # --- ğŸ‘‡ [ìˆ˜ì •ë¨] í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ê°œë³„ í–‰ìœ¼ë¡œ ë¶„ë¦¬ ---
    # ['chunk1', 'chunk2'] -> (row1, chunk1), (row1, chunk2)
    exploded_df = chunked_df.withColumn("chunk", explode(col("chunks"))) \
                            .select("id", "title", "url", "chunk")

    print("\në²¡í„° ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
    
    # --- ğŸ‘‡ [ìˆ˜ì •ë¨] Pandas UDFë¥¼ ì‚¬ìš©í•œ ë²¡í„° ë³€í™˜ ---
    # Pandas UDF í•¨ìˆ˜ ì •ì˜
    @pandas_udf(ArrayType(FloatType()))
    def embed_texts(texts: pd.Series) -> pd.Series:
        from sentence_transformers import SentenceTransformer
        # ëª¨ë¸ì€ ê° ì¼ê¾¼(worker)ë§ˆë‹¤ í•œ ë²ˆë§Œ ë¡œë“œë©ë‹ˆë‹¤.
        model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        # to_list()ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì— ì „ë‹¬
        embeddings = model.encode(texts.to_list(), show_progress_bar=False)
        # ê²°ê³¼ë¥¼ ë‹¤ì‹œ pandas Seriesë¡œ ë°˜í™˜
        return pd.Series(list(embeddings))

    # Pandas UDFë¥¼ ì ìš©í•˜ì—¬ 'embedding' ì»¬ëŸ¼ ìƒì„±
    embedded_df = exploded_df.withColumn("embedding", embed_texts(col("chunk")))

    # --- ğŸ‘‡ [ìˆ˜ì •ë¨] ChromaDB ì €ì¥ì„ ìœ„í•œ foreachPartition ---
    print("\nChromaDB ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    def save_to_chromadb(iterator):
        import chromadb
        db_client = chromadb.PersistentClient(path="/app/korea_db")
        collection = db_client.get_or_create_collection("korea_db")
        
        rows = list(iterator)
        if not rows: return

        ids = [f"{row.id}_{i}" for i, row in enumerate(rows)]
        embeddings = [row.embedding for row in rows]
        documents = [row.chunk for row in rows]
        metadatas = [{'title': row.title, 'url': row.url} for row in rows]
        
        collection.add(ids=ids, embeddings=embeddings, documents=documents, metadatas=metadatas)

    # íŒŒí‹°ì…˜ì„ ë‚˜ëˆ ì„œ ì €ì¥
    embedded_df.repartition(16).foreachPartition(save_to_chromadb)

    print("\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("í”„ë¡œì íŠ¸ í´ë” ì•ˆì— 'korea_db' í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    spark.stop()

if __name__ == "__main__":
    main()

